{
  
    
        "post0": {
            "title": "MixUp: concepts, usage, and implementations",
            "content": "In this post, we get to know the benefits of using mixup and how to implement it in practice. We dive into the official implementation of the MixUp paper and compare it to the implementation of the famous fastai library. . Mixup was introduced in this paper. Mixup is a simple technique for data augmentation, yet it has several advantages. As stated by the authors, mixup does: . regularize neural networks to favor simple linear behavior among training examples. | improve the generalization of state-of-the-art neural network architectures. | reduce the memorization of corrupt labels. | increase the robustness to adversarial examples. | stabilize the training of generative adversarial networks (GANs). | . The contributions from paper can be summarized by the following snapshot. . How can we implement mixup in practice? . The fastbook, by Jeremy Howard and Sylvain Gugger, does a great job in simplifying the concept. Essentially, mixup can be implemented with 4 simple steps; for each image: . Select another image from the dataset at random. | Pick a random weight. | Take a weighted sum (i.e., a linear combination) of the selected image with the original image. This will form the independent variable. | Take a weighted sum (using the same weight from step 2) of the labels of those two images. This will produce the dependent variable. | There is one additional requirement that the labels have to be one-hot encoded. . This sound really great! . Official implementation . Let&#39;s check the corresponding code from the official implementation of the paper. . The random weight is sampled from Beta distribution . lam = np.random.beta(alpha, alpha) . In practice, the random images are selected from within a mini batch rather than the whole dataset for convenience. . index = torch.randperm(batch_size) . The inputs are then mixep using a weighted sum with a random permutation of those same inputs . mixed_x = lam * x + (1 - lam) * x[index, :] . So far so good. . However, when we come to the point of mixing the labels we notice a deviation from the description above. . y_a, y_b = y, y[index] . we notice that labels are not being mixed when preparing the data. In addition, labels are not one-hot encoded in this particular case. We can confirm this by examining the loss function being used. . criterion = nn.CrossEntropyLoss() def mixup_criterion(criterion, pred, y_a, y_b, lam): return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b) . CrossEntropyLoss as defined in Pytorch does not accept one-hot encoded labels. We also notice a weighted sum of losses with respect to the original labels y_a and the permutation labels y_b, rather than the loss between predictions and a weighted some of labels. . what is wrong here? . Actually, there is nothing wrong! It turns out that this implementation is equivalent to the previous description. It is just easier to exploit this fact and use the existing labels and loss function (i.e., there is no need to convert labels to be one-hot encoded or change the loss function to accept such labels). . We will confirm this fact with a simplistic example. . Let pred be a batch of 4 predictions corresponding to mixed inputs mixed_x, y_a a batch of labels associated with the original inputs, and y_b a batch of labels associated with permuted inputs. . !pip install -Uqq fastai . |████████████████████████████████| 188 kB 9.3 MB/s |████████████████████████████████| 56 kB 2.6 MB/s . from fastai.basics import * . pred = tensor([[0.8, 0.2], [0.7,0.3], [0.4,0.6], [0.6, 0.4]]) pred . tensor([[0.8000, 0.2000], [0.7000, 0.3000], [0.4000, 0.6000], [0.6000, 0.4000]]) . y = tensor([0,0,1,1]) y . tensor([0, 0, 1, 1]) . batch_size = 4 index = torch.randperm(batch_size) index . tensor([0, 3, 2, 1]) . y_a, y_b = yb, yb[index] y_a, y_b . (tensor([0, 0, 1, 1]), tensor([0, 1, 1, 0])) . lam = tensor(0.3) lam . tensor(0.3000) . loss = nn.CrossEntropyLoss(reduction=&quot;none&quot;) def mixup_loss(loss, pred, y_a, y_b, lam): return lam * loss(pred, y_a) + (1 - lam) * loss(pred, y_b) . mixup_loss(loss, pred, y_a, y_b, lam) . tensor([0.4375, 0.7930, 0.5981, 0.6581]) . To this point, we obtained the loss as computed by the official implementation. We set the reduction to &quot;none&quot; in order to show all the components of the loss. However, the mean of those values should be computed in practice. . In the following section, we will implement the loss as described in fastbook and in the paper. . one_hot_y_a = tensor([[1, 0], [1, 0], [0, 1], [0, 1]]) one_hot_y_a . tensor([[1, 0], [1, 0], [0, 1], [0, 1]]) . one_hot_y_b = tensor([[1, 0], [0, 1], [0, 1], [1, 0]]) one_hot_y_b . tensor([[1, 0], [0, 1], [0, 1], [1, 0]]) . mixed_y = lam * one_hot_y_a + (1 - lam) * one_hot_y_b mixed_y . tensor([[1.0000, 0.0000], [0.3000, 0.7000], [0.0000, 1.0000], [0.7000, 0.3000]]) . We notice here that the first and the third labels did not change when compared with the original labels. This is because we are mixing with the same labels in these cases. . Next, we define a version of the cross entropy loss that accepts one-hot encoded labels (or a weighted sum of one-hot encoded labels to be precise; those are also called soft labels as opposed to hard labels i.e., zero or one). . def one_hot_CELoss(pred, target): logsoftmax = nn.LogSoftmax() return torch.sum(-target * logsoftmax(pred), dim=1) . one_hot_CELoss(pred, mixed_y) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument. This is separate from the ipykernel package so we can avoid doing imports until . tensor([0.4375, 0.7930, 0.5981, 0.6581]) . test_close(one_hot_CELoss(pred, mixed_y) , mixup_loss(loss, pred, y_a, y_b, lam)) . Hooray! The results are matching. . Fastai Implementation . Fastai follows the official implementation of drawing the random images from the mini batch. The code looks a bit different as fastai uses some functionalities of the L class from fastcore. . shuffle = torch.randperm(self.y.size(0)).to(self.x.device) xb1,self.yb1 = tuple(L(self.xb).itemgot(shuffle)),tuple(L(self.yb).itemgot(shuffle)) . Similarly, fastai&#39;s implementation mixes the inputs and takes a mix of losses instead of mixing one-hot encoded labels. . self.learn.xb = tuple(L(xb1,self.xb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=nx_dims-1))) . loss = torch.lerp(lf(pred,*self.yb1), lf(pred,*yb), self.lam) . But wait a minute! . Looks like there is no weighted sum in here. We notice the use of torch.lerp instead. If you look up the documentation of torch.lerp, you will find out the it does something like this: . torch.lerp(input, end, weight, *, out=None) . $out_{i} = input_{i} + weight_{i} times (end_{i}-input_{i})$ . with simple refactoring we can see that: . $out_{i} = input_{i} + weight_{i} times end_{i} - weight_{i} times input_{i}$ . $out_{i} = (1-weight_{i}) times input_{i} + weight_{i} times end_{i}$ . which is the weighted sum of the two tensors input and end. . But why not simply use a clear weighted sum instead of torch.lerp?! . The answer is performance! Pytorch has very optimized low level implementations for certain operations which is usually much faster. Let&#39;s confirm this with a little experiment. . a = torch.randn((64,400,400)) b = torch.randn((64,400,400)) . %timeit -n100 ((1-lam)*a + lam*b) . 100 loops, best of 5: 32.6 ms per loop . %timeit -n100 torch.lerp(a,b,lam) . 100 loops, best of 5: 12.7 ms per loop . 32.6/12.7 . 2.566929133858268 . we can see over 2.5X speed-up boost when using torch.lerp. And since we are talking about fastai, it&#39;s better be using torch.lerp. .",
            "url": "https://feras-oughali.github.io/blog/fastai/mixup/2021/08/10/mixup.html",
            "relUrl": "/fastai/mixup/2021/08/10/mixup.html",
            "date": " • Aug 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How to Prepare for Fellowship.ai Application",
            "content": "Being midway through my journey with Fellowship.ai, I decided to share some aspects that might help those who are considering applying to the program. . I first came across Fellowship.ai about a couple of years back when I was going through fast.ai courses. It sounded really compelling, but it was a big commitment for me at the time as the program was in-person and at a handful of cities. This meant that I had to relocate to one of those cities for the entire period of the program. . Luckily, this has changed and the program is now fully remote. This makes it a very convenient option for aspiring data scientists and machine learning engineers. . What is Fellowship.ai? . Fellowship.ai is a highly competitive program that provides you with hands-on experience working on real-world projects spanning across various tasks in an environment that resembles actual industry setting. . What do you gain by joining the fellowship? . In addition to the hands-on experience, you get to practice agile development methodology, work with a team of fellows with diverse backgrounds, and access to a network of experienced mentors. As an added bonus, you get to participate in the AI reading group where you present and get to know about cutting-edge research papers. Moreover, externships are typically offered by Launchpad.AI to top performing fellows. . What experience is required? . As mentioned earlier, you will be working on implementing leading-edge solutions to solid business or research problems. Hence, you are expected to be well-versed in writing code (preferably in Python). Besides, you are expected to research and explore innovative ideas. Therefore, you should be able to read scientific papers and adopt code from open-source projects implementing such techniques. . Looking at my situation a couple of years back, I don’t think I would have made a great candidate. . There is of course a lot of room for learning and improvement along the way, but you should be well equipped so you don’t feel overwhelmed throughout the journey. If you don’t feel ready yet, there is nothing wrong about spending a few more months to harness few extra concepts and sharpen your coding skills before you apply. This way you can make the best of the program instead of being discouraged by trying to catch up with your teammates. . Attempting a challenge and submitting your application . Once you are ready for submitting your application, you will need to pick and submit a challenge. There will be few challenges to select from spanning across various tasks like computer vision, NLP, etc. My advice is to pick something you are most familiar and passionate about. This is your time to shine and show your potential. . There are a set of hints for hacking the challenge which are listed by the organizers. Make sure to critically consider those guidelines while developing your solution. The challenge statement might look somehow concise or straightforward (at least it sounded like that to me), but in the end I realized this might have been intentional to leave the floor for innovations rather than spelling out all the required steps. In the next section, I am going to share another important resource that you should take to your advantage. . In addition, you will need to prepare a resume and submit a short video that introduces yourself, your motivation for joining the program, and an overview of your proposed solution to the challenge. This video is supposed to replace an interview. So, take that into consideration while conveying the required points. You don’t need to reiterate the solution to the challenge, rather you can highlight some interesting facts about your approach to the problem. . The program is offered three times a year; make sure to choose the best period that suits your circumstances. The summer session might be a good choice if you are still studying. . The admission happens on rolling bases. Submit as early as possible to maximize your chance of acceptance. . Handy resources . AMA sessions: Fellowship.ai regularly hosts online sessions to answer questions related to the program. This was a very crucial resource for me. I was almost ready to submit my solution to the challenge when I decided to join one of those sessions. Fortunately, the feedback I got from the session led me to further improve my approach by exploring new techniques that I have not used before. It took me almost three weeks to revise the submission, but this had paid off and I received the offer to join the program in about two weeks time. . | Guidelines: There is a list of guidelines for submitting a compelling solution on the program’s website. As you progress with your development, revisit those points and you will probably find new ways to enhance your methodology. | Fast.ai courses: In my opinion, fast.ai courses are a great way to get started with deep learning. Those courses get you up to speed with foundational concepts of machine learning as well as state-of-the-art techniques in applying deep learning to various tasks in computer vision, NLP, tabular data, etc. | . Final remarks . I feel very content with my journey in the program so far. There are a lot of things that I have learned along the way. If you would like to know more how a typical day is for fellows, please stay tuned. I will be sharing my review of the program and highlighting what I have learned once the program ends. .",
            "url": "https://feras-oughali.github.io/blog/fellowship/2021/07/13/FellowshipAI-Application.html",
            "relUrl": "/fellowship/2021/07/13/FellowshipAI-Application.html",
            "date": " • Jul 13, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Feras Chikh Oughali. This is where I share some thoughts. .",
          "url": "https://feras-oughali.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://feras-oughali.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}