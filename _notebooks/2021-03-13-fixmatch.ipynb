{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FixMatch for Semi-Supervised Learning\n",
    "> pytorch implementation of FixMatch paper as a fastai callback\n",
    "\n",
    "- toc: false \n",
    "- badges: true\n",
    "- comments: false\n",
    "- categories: [semi]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work is ported from FixMatch pytorch implementation [repo](https://github.com/kekmodel/FixMatch-pytorch) by Jungdae Kim. For more details about FixMatch, have a look at this excellent blog post [The Illustrated FixMatch for Semi-Supervised Learning](https://amitness.com/2020/03/fixmatch-semi-supervised/). FixMatch paper can be found [here](https://arxiv.org/abs/2001.07685)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this callback you need:  \n",
    "- A FixMatchTransform: This is a transform that performs weak and strong augmentations on each of the unlabeled images as shown above. This can be used when creating the dataset. \n",
    "\n",
    "- A pytorch dataloader for the transformed unlabeled images. This dataloader is passed to the `FixMatch` callback.\n",
    "\n",
    "In this implementation, FixMatchTransform utilizes `rand_augment_transform` from *timm* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data.auto_augment import rand_augment_transform\n",
    "\n",
    "rand_aug = rand_augment_transform(\n",
    "    config_str='rand-m9-mstd0.5', \n",
    "    hparams={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixMatchTransform():\n",
    "    def __init__(self, mean, std):\n",
    "        self.weak = transforms.Compose([transforms.RandomHorizontalFlip()])\n",
    "        self.strong = rand_aug\n",
    "        self.normalize = transforms.Compose([transforms.ToTensor(),\n",
    "                                             transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        weak = self.weak(x)\n",
    "        strong = self.strong(x)\n",
    "        return self.normalize(weak), self.normalize(strong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the unlabeled dataset. STL-10 dataset will be used throughout this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stl10_mean = (0.4914, 0.4822, 0.4465)\n",
    "stl10_std = (0.2471, 0.2435, 0.2616)\n",
    "\n",
    "unlabeled_dataset = torchvision.datasets.STL10(root='/kaggle/input/stl10/', split='unlabeled', \n",
    "                                               transform=FixMatchTransform(stl10_mean,stl10_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "mu = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_trainloader = torch.utils.data.DataLoader(\n",
    "        unlabeled_dataset,\n",
    "        batch_size=batch_size*mu,\n",
    "        shuffle=True,\n",
    "        drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the FixMatch callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixMatch(Callback):\n",
    "    \n",
    "    run_valid = False\n",
    "    \n",
    "    def __init__(self, unlabeled_trainloader, threshold=0.95, lambda_u=1.0, T=1.0):\n",
    "        self.unlabeled_trainloader = unlabeled_trainloader\n",
    "        self.unlabeled_iter = iter(self.unlabeled_trainloader)\n",
    "        self.threshold = threshold\n",
    "        self.lambda_u = lambda_u\n",
    "        self.T = T\n",
    "        \n",
    "    def before_train(self):\n",
    "        self.old_lf,self.learn.loss_func = self.learn.loss_func,self.lf\n",
    "    \n",
    "    def after_train(self):\n",
    "        self.learn.loss_func = self.old_lf\n",
    "        \n",
    "    def before_batch(self):\n",
    "        try:\n",
    "            (self.inputs_u_w, self.inputs_u_s), _ = self.unlabeled_iter.next()\n",
    "        except:\n",
    "            self.unlabeled_iter = iter(self.unlabeled_trainloader)\n",
    "            (self.inputs_u_w, self.inputs_u_s), _ = self.unlabeled_iter.next()\n",
    "        self.inputs_u_w, self.inputs_u_s = to_device(self.inputs_u_w),to_device(self.inputs_u_s)\n",
    "        self.learn.xb = tuple(L(torch.cat((self.learn.xb[0], self.inputs_u_w, self.inputs_u_s))))\n",
    "        \n",
    "    def after_pred(self):\n",
    "        self.logits_x = self.pred[:self.dls.train.bs]\n",
    "        self.logits_u_w, self.logits_u_s = self.pred[self.dls.train.bs:].chunk(2)\n",
    "        \n",
    "    def lf(self, pred, *yb):\n",
    "        if not self.training: return self.old_lf(pred, *yb)\n",
    "        Lx = self.old_lf(self.logits_x, *yb)\n",
    "        pseudo_label = torch.softmax(self.logits_u_w.detach()/self.T, dim=-1)\n",
    "        max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "        mask = max_probs.ge(self.threshold).float()\n",
    "        with NoneReduce(self.old_lf) as lf:\n",
    "            uloss = lf(self.logits_u_s, targets_u) * mask\n",
    "        Lu = reduce_loss(uloss, 'mean')\n",
    "        return Lx + self.lambda_u * Lu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we construct the callback and pass it to the learner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, metrics=accuracy, loss_func=CrossEntropyLossFlat(), opt_func=SGD, \n",
    "                cbs=[FixMatch(unlabeled_trainloader)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we follow the standard procedure for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, 3e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
